{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Jaxoplanet core from scratch\n",
    "\n",
    "Inspired by the [autodidax tutorial](https://jax.readthedocs.io/en/latest/autodidax.html) from the JAX documentation, in this tutorial we work through implementing some of the core `jaxoplanet` functionality from scratch, to demonstrate and discuss the choices made within the depths of the codebase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Solving Kepler's equation\n",
    "\n",
    "One core piece of infrastructure provided by `jaxoplanet` is a function to solve Kepler's equation\n",
    "\n",
    "$$\n",
    "M = E - e \\sin(E)\n",
    "$$\n",
    "\n",
    "for the eccentric anomaly $E$ as a function of the eccentricity $e$ and mean anomaly $M$.\n",
    "There is a lot of literature dedicated to solving this equation efficiently and robustly, and we won't get into all the details here, but there are a few points we should highlight:\n",
    "\n",
    "1. The methods that are most commonly used in astrophysics to solve this equation are all iterative, using some sort of root finding scheme. While these methods can work well, they tend to be less computationally efficient than non-iterative approaches. Even more importantly for our purposes, non-iterative methods are better suited to massively parallel compute architectures like GPUs. These non-iterative methods typically have a two step form: (i) make a good initial guess (\"starter\") for $E$, then (ii) use a high order root finding update to refine this estimate. \n",
    "\n",
    "2. In most Python codes, the Kepler solver is offloaded to a compiled library, but we will find that we can get comparable performance just using JAX, and relying on its JIT compilation to accelerate the computation.\n",
    "\n",
    "With these points in mind, we can implement the solver that is included with `jaxoplanet`.\n",
    "\n",
    "### Pure-Python (+JAX) solver\n",
    "\n",
    "The solver that we use in `jaxoplanet` is based on the method from [Markley (1995)](https://ui.adsabs.harvard.edu/abs/1995CeMDA..63..101M/abstract).\n",
    "It is possible to get better CPU performance using more sophisticated methods (see, for example, [Raposo-Pulido & PelÃ¡ez 2017](https://ui.adsabs.harvard.edu/abs/2017MNRAS.467.1702R/abstract), and the follow up [Brandt et al. 2021](https://ui.adsabs.harvard.edu/abs/2021AJ....162..186B/abstract)), but these methods are somewhat harder to implement efficiently using JAX's programming model, and we expect that they would not be so easily extensible to GPU or TPU acceleration.\n",
    "Regardless, we find that JAX's JIT compilation allows us to achieve nearly state-of-the-art runtime performance, even with this simple method.\n",
    "\n",
    "First we implement a \"starter\" function which uses Markley's approximation to estimate $E$ as a function of $M$ and $e$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "\n",
    "def kepler_starter(mean_anom, ecc):\n",
    "    ome = 1 - ecc\n",
    "    M2 = jnp.square(mean_anom)\n",
    "    alpha = 3 * jnp.pi / (jnp.pi - 6 / jnp.pi)\n",
    "    alpha += 1.6 / (jnp.pi - 6 / jnp.pi) * (jnp.pi - mean_anom) / (1 + ecc)\n",
    "    d = 3 * ome + alpha * ecc\n",
    "    alphad = alpha * d\n",
    "    r = (3 * alphad * (d - ome) + M2) * mean_anom\n",
    "    q = 2 * alphad * ome - M2\n",
    "    q2 = jnp.square(q)\n",
    "    w = jnp.square(jnp.cbrt(jnp.abs(r) + jnp.sqrt(q2 * q + r * r)))\n",
    "    return (2 * r * w / (jnp.square(w) + w * q + q2) + mean_anom) / d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Then we implement a third order Householder update to refine this estimate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kepler_refiner(mean_anom, ecc, ecc_anom):\n",
    "    ome = 1 - ecc\n",
    "    sE = ecc_anom - jnp.sin(ecc_anom)\n",
    "    cE = 1 - jnp.cos(ecc_anom)\n",
    "\n",
    "    f_0 = ecc * sE + ecc_anom * ome - mean_anom\n",
    "    f_1 = ecc * cE + ome\n",
    "    f_2 = ecc * (ecc_anom - sE)\n",
    "    f_3 = 1 - f_1\n",
    "    d_3 = -f_0 / (f_1 - 0.5 * f_0 * f_2 / f_1)\n",
    "    d_4 = -f_0 / (f_1 + 0.5 * d_3 * f_2 + (d_3 * d_3) * f_3 / 6)\n",
    "    d_42 = d_4 * d_4\n",
    "    dE = -f_0 / (f_1 + 0.5 * d_4 * f_2 + d_4 * d_4 * f_3 / 6 - d_42 * d_4 * f_2 / 24)\n",
    "\n",
    "    return ecc_anom + dE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Putting these together, we can construct a solver function which includes some extra bookkeeping to handle the range reduction of the inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "@jnp.vectorize\n",
    "def kepler_solver_impl(mean_anom, ecc):\n",
    "    mean_anom = mean_anom % (2 * jnp.pi)\n",
    "\n",
    "    # We restrict to the range [0, pi)\n",
    "    high = mean_anom > jnp.pi\n",
    "    mean_anom = jnp.where(high, 2 * jnp.pi - mean_anom, mean_anom)\n",
    "\n",
    "    # Solve\n",
    "    ecc_anom = kepler_starter(mean_anom, ecc)\n",
    "    ecc_anom = kepler_refiner(mean_anom, ecc, ecc_anom)\n",
    "\n",
    "    # Re-wrap back into the full range\n",
    "    ecc_anom = jnp.where(high, 2 * jnp.pi - ecc_anom, ecc_anom)\n",
    "\n",
    "    return ecc_anom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "And that's it! Now we have a solver for Kepler's equation that we can use from JAX.\n",
    "\n",
    "Some notes:\n",
    "\n",
    "1. We've called this function `kepler_solver_impl` rather than `kepler_solver`, for reasons that we will get into shortly.\n",
    "2. This function was decorated with the [`jax.numpy.vectorize` function](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.vectorize.html) which, while not strictly necessary in this case, is useful because it signals that we have implemented our solver for scalar inputs and we let JAX handle the vectorization to arrays of different shapes. Unlike `numpy.vectorize`, the JAX version incurs no runtime overhead when vectorizing.\n",
    "\n",
    "To check to make sure that our implementation works, let's make sure that that our method actually solves the equation of interest.\n",
    "We start by generating a grid of known eccentric anomalies and computing the corresponding array of mean anomalies using Kepler's equation.\n",
    "Then we make sure that our solver returns the correct values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ecc = 0.5\n",
    "true_ecc_anom = jnp.linspace(0, 2 * jnp.pi, 50_000)[:-1]\n",
    "mean_anom = true_ecc_anom - ecc * jnp.sin(true_ecc_anom)\n",
    "\n",
    "calc_acc_anom = kepler_solver_impl(mean_anom, ecc)\n",
    "\n",
    "plt.plot(true_ecc_anom, jnp.abs(calc_acc_anom - true_ecc_anom), \"k\")\n",
    "plt.axhline(0, color=\"k\")\n",
    "plt.xlabel(\"eccentric anomaly\")\n",
    "plt.ylabel(r\"error from Kepler solver\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "We enabled double precision above and we can see here that the results are correct to within $<10^{-15}$ of absolute error, which should do just fine for any practical applications.\n",
    "This error does increase for large eccentricities, but even then it should always perform better than $\\sim 10^{-12}$, and a full analysis is beyond the scope of this tutorial.\n",
    "\n",
    "It's also worth testing the performance of this implementation.\n",
    "Since we execute these tutorials on Read the Docs, the specific results here will depend on the actual allocated hardware, but here's how you would benchmark this operation on your system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a large number of test values to minimize effects of Python overhead\n",
    "mean_anom_bench = jnp.linspace(0, 2 * jnp.pi, 500_000)[:-1]\n",
    "%timeit kepler_solver_impl(mean_anom_bench, ecc).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "On my 2020 M1 MacBook Pro, I get a runtime of `7 ms` per loop, and in each loop we're solving Kepler's equation 500,000 times, so that means that each solve costs about `14 ns`.\n",
    "For comparison, on the same system, the Kepler solver from [RadVel](https://github.com/California-Planet-Search/radvel/blob/b279a88710b628f70c9de4045f86e2f864674dc5/src/_kepler.pyx#L24) which is implemented in C/Cython takes `35 ms` per loop.\n",
    "\n",
    "We should note that the point here is not to pick on RedVel, and let's be clear that this is by no means a complete comparison!\n",
    "Instead, we want to highlight that in some cases JIT-compiled JAX code can get similar performance to that provided by optimized low level code with Python bindings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, sharex=True)\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(\n",
    "    mean_anom, jax.vmap(jax.grad(kepler_solver_impl), in_axes=(0, None))(mean_anom, ecc)\n",
    ")\n",
    "ax.set_ylabel(\"dE / dM\")\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(\n",
    "    mean_anom,\n",
    "    jax.vmap(jax.grad(kepler_solver_impl, argnums=1), in_axes=(0, None))(\n",
    "        mean_anom, ecc\n",
    "    ),\n",
    ")\n",
    "ax.set_xlabel(\"mean anomaly\")\n",
    "ax.set_ylabel(\"dE / de\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathrm{d}M = \\mathrm{d}E (1 - e \\cos E) - \\mathrm{d}e \\sin E\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial M} = \\frac{1}{1 - e \\cos E}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial e} = \\frac{\\sin E}{1 - e \\cos E}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.interpreters import ad\n",
    "\n",
    "\n",
    "@jax.custom_jvp\n",
    "def kepler_solver(mean_anom, ecc):\n",
    "    return kepler_solver_impl(mean_anom, ecc)\n",
    "\n",
    "\n",
    "@kepler_solver.defjvp\n",
    "def kepler_solver_jvp(primals, tangents):\n",
    "    mean_anom, ecc = primals\n",
    "    d_mean_anom, d_ecc = tangents\n",
    "\n",
    "    # Run the solver from above to compute `ecc_anom`\n",
    "    ecc_anom = kepler_solver(mean_anom, ecc)\n",
    "\n",
    "    # Propagate the derivatives using the implicit function theorem\n",
    "    dEdM = 1 / (1 - ecc * jnp.cos(ecc_anom))\n",
    "    dEde = jnp.sin(ecc_anom) * dEdM\n",
    "    d_ecc_anom = dEdM * make_zero(d_mean_anom) + dEde * make_zero(d_ecc)\n",
    "\n",
    "    return ecc_anom, d_ecc_anom\n",
    "\n",
    "\n",
    "def make_zero(tan):\n",
    "    # This is a helper function to handle symbolic zeros (i.e. parameters\n",
    "    # that are not being differentiated)\n",
    "    if type(tan) is ad.Zero:\n",
    "        return ad.zeros_like_aval(tan.aval)\n",
    "    else:\n",
    "        return tan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, sharex=True)\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(mean_anom, jax.vmap(jax.grad(kepler_solver), in_axes=(0, None))(mean_anom, ecc))\n",
    "ax.set_ylabel(\"dE / dM\")\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(\n",
    "    mean_anom,\n",
    "    jax.vmap(jax.grad(kepler_solver, argnums=1), in_axes=(0, None))(mean_anom, ecc),\n",
    ")\n",
    "ax.set_xlabel(\"mean anomaly\")\n",
    "ax.set_ylabel(\"dE / de\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
